{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloud-volume\n",
      "  Downloading https://files.pythonhosted.org/packages/96/39/dac6a9da64502176b00e445a591d1b6a23484fb585330073a4f965d16f97/cloud_volume-0.23.0-py2.py3-none-any.whl (65kB)\n",
      "\u001b[K    100% |################################| 71kB 2.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting python-jsonschema-objects==0.2.1 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/1b/2ea3493d53da8fb2475fd0f11c5cd04c2910f12b5678af11cd9ee0de8727/python_jsonschema_objects-0.2.1-py2.py3-none-any.whl\n",
      "Collecting posix-ipc==1.0.4 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/3e/54217da71aa26b488295d878df4d3132093253b4ae5798ac66fcb6921ef0/posix_ipc-1.0.4.tar.gz (78kB)\n",
      "\u001b[K    100% |################################| 81kB 3.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting psutil==5.4.3 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/e1/600326635f97fee89bf8426fef14c5c29f4849c79f68fd79f433d8c1bd96/psutil-5.4.3.tar.gz (412kB)\n",
      "\u001b[K    100% |################################| 419kB 2.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting six==1.10.0 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b44607ab535be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl\n",
      "Collecting requests>=2.18.4 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K    100% |################################| 92kB 4.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting google-auth>=1.0.2 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/06/6e6d5bfa4d23ee40efd772d6b681a7afecd859a9176e564b8c329382370f/google_auth-1.5.0-py2.py3-none-any.whl (65kB)\n",
      "\u001b[K    100% |################################| 71kB 5.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.5/dist-packages (from cloud-volume)\n",
      "Collecting boto3>=1.4.7 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/1c/898ab9025a1725d15c3b121f6c91642a2535acc5d363acb328d6b37ff6d1/boto3-1.7.40-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |################################| 133kB 4.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud==0.32.0 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/49/03321cbc697d4aa97f74d87f1dc865ecdf05e4feb47f1c49b50573163a9c/google_cloud-0.32.0-py2.py3-none-any.whl\n",
      "Collecting intern (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/85/aeb89f2a4f0339af67b7ad8d7330780e67aab1360e6444336a824f14eb80/intern-0.9.6-py2.py3-none-any.whl (86kB)\n",
      "\u001b[K    100% |################################| 92kB 5.4MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting backports-abc==0.5 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/56/6f3ac1b816d0cd8994e83d0c4e55bc64567532f7dc543378bd87f81cebc7/backports_abc-0.5-py2.py3-none-any.whl\n",
      "Collecting json5==0.5.1 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/cf/050247dcd8000a8f2413d7932281991acba5bb72270f5f39d0a22c9b3074/json5-0.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.5/dist-packages (from cloud-volume)\n",
      "Collecting pytest>=3.3.1 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/75/e79b66c9fe6166a90004bb8fb02bab06213c3348e93f3be41d7eaf625554/pytest-3.6.1-py2.py3-none-any.whl (194kB)\n",
      "\u001b[K    100% |################################| 194kB 3.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.3.0 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/c4/8a35f5af5f26040ae7f3d521875e43429d2955d598fa3f2d0b6b88133bb1/protobuf-3.6.0-cp35-cp35m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K    100% |################################| 7.1MB 212kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |################################| 51kB 12.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3[secure] (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 9.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet==3.0.4 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 9.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity==4.4.0 (from cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/0c/9c3adb8f8a515201394c18d49daed795e61f03f9bcb24bbf09da6bbb704a/tenacity-4.4.0.tar.gz\n",
      "Collecting jsonschema==2.3.0 (from python-jsonschema-objects==0.2.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/1c/52812523feebd744ac0e268224b5bae048b1559c88728f3628d169427947/jsonschema-2.3.0-py2.py3-none-any.whl\n",
      "Collecting inflection==0.2.0 (from python-jsonschema-objects==0.2.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/17/e063022e2b77cb9f4a83ed4089e5b0debdddbc6b685c679f387b657c73c1/inflection-0.2.0.tar.gz\n",
      "Collecting Markdown==2.4 (from python-jsonschema-objects==0.2.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/5d/d259c3b20aaafade22b3fb220fcfeee03124562ace2c6ddba7a5474a76c5/Markdown-2.4.tar.gz (260kB)\n",
      "\u001b[K    100% |################################| 266kB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandocfilters==1.2 (from python-jsonschema-objects==0.2.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/ce/4c38a7e4f0eec5c071ff94d322f37f75af1f20b4c443d023c423219a2add/pandocfilters-1.2.tar.gz\n",
      "Collecting idna<2.8,>=2.5 (from requests>=2.18.4->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |################################| 61kB 11.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests>=2.18.4->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/e6/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |################################| 153kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.2->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/51/bcd96bf6231d4b2cc5e023c511bee86637ba375c44a6f9d1b4b7ad1ce4b9/pyasn1_modules-0.2.1-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |################################| 61kB 11.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cachetools>=2.0.0 (from google-auth>=1.0.2->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/58/cbee863250b31d80f47401d04f34038db6766f95dea1cc909ea099c7e571/cachetools-2.1.0-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from google-auth>=1.0.2->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |################################| 51kB 11.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3>=1.4.7->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting botocore<1.11.0,>=1.10.40 (from boto3>=1.4.7->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/6f/e9c3981f8b7e93bfa4461b754563b0e917968947920d0bdcf2a7dcf77da2/botocore-1.10.40-py2.py3-none-any.whl (4.3MB)\n",
      "\u001b[K    100% |################################| 4.3MB 357kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3>=1.4.7->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |################################| 61kB 12.2MB/s ta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting google-cloud-logging<1.5dev,>=1.4.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/a6/c6f0fd13fbe70b007b105d42b1914e186dea60bc58628f694ae2b369d20b/google_cloud_logging-1.4.0-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |################################| 51kB 13.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-translate<1.4dev,>=1.3.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/40/32/e17ba4b6586c31afe08a82fd0e3437906757a5fedb29e44d695721ffdbb8/google_cloud_translate-1.3.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-bigquery-datatransfer<0.2dev,>=0.1.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/6b/636a8702adad77b38f3df951be3c846dfcd8c79854ed2d71654be5602ab8/google_cloud_bigquery_datatransfer-0.1.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-firestore<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/3c/6f25b101d5c2584bfb6db74d75b9a08094a9986623acd372336f72ccd084/google_cloud_firestore-0.28.0-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |################################| 153kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-pubsub<0.31dev,>=0.30.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/a2/4e0576083986296d21c530738530bd9de4dcb0fd4728e963ad8ff7e1620f/google_cloud_pubsub-0.30.1-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |################################| 92kB 12.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigquery<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/f6/193729cb124770c710b9eab14f2fe2c3c7b285cc7b8cc70f48c162616e09/google_cloud_bigquery-0.28.0-py2.py3-none-any.whl (64kB)\n",
      "\u001b[K    100% |################################| 71kB 13.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-storage<1.7dev,>=1.6.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/13/131c4d6b72411bcd56ab82a70a256d961e8d87e7b6356c12791c0003765d/google_cloud_storage-1.6.0-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |################################| 61kB 12.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-videointelligence<1.1dev,>=1.0.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/e4/57ffb1ef3fb003815789c684b3e7abd9f5df6f66471b08d5213bea6900d4/google_cloud_videointelligence-1.0.1-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |################################| 61kB 12.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-dns<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/57/16/0098588f04399897209d4e854e430db4c8bb540ec84b9431ec30c320ac7a/google_cloud_dns-0.28.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-spanner<0.30dev,>=0.29.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/3f/d1771299f1df521dd918830511f03d1df9ee27d5a2e20252811feed39060/google_cloud_spanner-0.29.0-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 10.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-speech<0.31dev,>=0.30.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/70/a5/f4722af1873e59e84ee1f6d28eb44cb697174f6ee4433a1cc89bd76f9890/google_cloud_speech-0.30.0-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |################################| 51kB 13.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-trace<0.18dev,>=0.17.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/44/261efe8430c05509d7cadcb1bdd87745afb7a2d999f5fa41b0ee92ee6637/google_cloud_trace-0.17.0-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |################################| 71kB 12.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigtable<0.29dev,>=0.28.1 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/03/38/97d17a7fefc8596d49be95b409f9476d5c31bd9fb00d9c243652b883099f/google_cloud_bigtable-0.28.1-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |################################| 92kB 12.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-vision<0.30dev,>=0.29.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/56/49a9819835ef58b6547af22bb8d70677b24f3219eb47d35aa61428008474/google_cloud_vision-0.29.0-py2.py3-none-any.whl (71kB)\n",
      "\u001b[K    100% |################################| 81kB 13.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-resource-manager<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/bf/ed87e187c214154dd8a630c779dd103e9080026c9b046e9d0c52694f332c/google_cloud_resource_manager-0.28.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-monitoring<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/99/67195b899d8e1189807130062ba68018533677879590673028671fe4c570/google_cloud_monitoring-0.28.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-core<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/41/ae2418b4003a14cf21c1c46d61d1b044bf02cf0f8f91598af572b9216515/google_cloud_core-0.28.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-language<1.1dev,>=1.0.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/f7/2fa4d0ea8381fccd182b4b9c408dae780c59bcfd60dde016b90a552f2cd2/google_cloud_language-1.0.2-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |################################| 61kB 11.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-runtimeconfig<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/df/fd/12949f877975e2583cd3cd192e73759a3ba5b4f59974a669eaecc6f375df/google_cloud_runtimeconfig-0.28.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-datastore<1.5dev,>=1.4.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/bc/2cee1ac9cc70764845a6ff73e4e5dc50aef1225aac417461e1ef878ed38e/google_cloud_datastore-1.4.0-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |################################| 51kB 12.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-container<0.2dev,>=0.1.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/40/8e603a2acf6e1c81272f29cfaff2db60436545f8667b32853e58b160cb39/google_cloud_container-0.1.1-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |################################| 51kB 11.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-error-reporting<0.29dev,>=0.28.0 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/f3/4071bc943b48e70e41bfc56ed5ecbb5a7eddd8312f10c25a8834669ea20d/google_cloud_error_reporting-0.28.0-py2.py3-none-any.whl\n",
      "Collecting google-api-core<0.2.0dev,>=0.1.2 (from google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/65/6237293db4fbf6f0bcf7c2b67c63e4dc4837c631f194064ae84957cd0313/google_api_core-0.1.4-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |################################| 51kB 11.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting blosc==1.4.4 (from intern->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/19/cc48ff2219cd479ab4c867eebaf865b32cbe67d8607c1f7b02c3e749ca2f/blosc-1.4.4.tar.gz (613kB)\n",
      "\u001b[K    100% |################################| 614kB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock (from intern->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |################################| 61kB 11.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting nose2 (from intern->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/46/a389a65237d0520bb4a98fc174fdf6568ad9dcc79b9c1d1f30afc6776031/nose2-0.7.4.tar.gz (141kB)\n",
      "\u001b[K    100% |################################| 143kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting more-itertools>=4.0.0 (from pytest>=3.3.1->cloud-volume)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/85/40/90c3b0393e12b9827381004224de8814686e3d7182f9d4182477f600826d/more_itertools-4.2.0-py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |################################| 51kB 12.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.4.0 (from pytest>=3.3.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n",
      "Collecting pluggy<0.7,>=0.5 (from pytest>=3.3.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/65/ded3bc40bbf8d887f262f150fbe1ae6637765b5c9534bd55690ed2c0b0f7/pluggy-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from pytest>=3.3.1->cloud-volume)\n",
      "Collecting py>=1.5.0 (from pytest>=3.3.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/a5/f77982214dd4c8fd104b066f249adea2c49e25e8703d284382eb5e9ab35a/py-1.5.3-py2.py3-none-any.whl (84kB)\n",
      "\u001b[K    100% |################################| 92kB 12.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting atomicwrites>=1.0 (from pytest>=3.3.1->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/e8/cd6375e7a59664eeea9e1c77a766eeac0fc3083bb958c2b41ec46b95f29c/atomicwrites-1.1.5-py2.py3-none-any.whl\n",
      "Collecting monotonic>=0.6 (from tenacity==4.4.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.2->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |################################| 81kB 12.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.11.0,>=1.10.40->boto3>=1.4.7->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K    100% |################################| 552kB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.5/dist-packages (from botocore<1.11.0,>=1.10.40->boto3>=1.4.7->cloud-volume)\n",
      "Collecting gapic-google-cloud-logging-v2<0.92dev,>=0.91.0 (from google-cloud-logging<1.5dev,>=1.4.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/a8/2fcb30c255243d0b5b1a1c4b3cd0a73ca87c82f8a6673be60fb003a6e184/gapic-google-cloud-logging-v2-0.91.3.tar.gz\n",
      "Collecting google-gax<0.16dev,>=0.15.7 (from google-cloud-firestore<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/b4/ff312fa42f91535c67567c1d08e972db0e7c548e9a63c6f3bcc5213b32fc/google_gax-0.15.16-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |################################| 51kB 11.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from google-cloud-pubsub<0.31dev,>=0.30.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
      "Collecting google-resumable-media>=0.2.1 (from google-cloud-bigquery<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/95/2e4020a54366423ddba715f89fb7ca456c8f048b15cada6cd6a54cf10e8c/google_resumable_media-0.3.1-py2.py3-none-any.whl\n",
      "Collecting googleapis-common-protos[grpc]<2.0dev,>=1.5.2 (from google-cloud-vision<0.30dev,>=0.29.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/03/d25bed04ec8d930bcfa488ba81a2ecbf7eb36ae3ffd7e8f5be0d036a89c9/googleapis-common-protos-1.5.3.tar.gz\n",
      "Collecting gapic-google-cloud-datastore-v1<0.16dev,>=0.15.0 (from google-cloud-datastore<1.5dev,>=1.4.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/9c/6d69a6e6551006c4e87192a396b983421a6978bc57657619bc998a264b31/gapic-google-cloud-datastore-v1-0.15.3.tar.gz\n",
      "Collecting gapic-google-cloud-error-reporting-v1beta1<0.16dev,>=0.15.0 (from google-cloud-error-reporting<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/03/fb/903f4cb88a442e05ec789329aef76da0d3f841a66909e9b5bc39527e0ef7/gapic-google-cloud-error-reporting-v1beta1-0.15.3.tar.gz\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from google-api-core<0.2.0dev,>=0.1.2->google-cloud==0.32.0->cloud-volume)\n",
      "Collecting pbr>=0.11 (from mock->intern->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/5d/c196041ffdf3e34ba206db6d61d1f893a75e1f3435699ade9bd65e089a3d/pbr-4.0.4-py2.py3-none-any.whl (98kB)\n",
      "\u001b[K    100% |################################| 102kB 12.0MB/s a 0:00:01\n",
      "\u001b[?25hCollecting coverage>=4.4.1 (from nose2->intern->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/c0/8047b7cbbcdbd7d21f8d68126196b7915da892c5af3d1a99dba082d33ec0/coverage-4.5.1-cp35-cp35m-manylinux1_x86_64.whl (202kB)\n",
      "\u001b[K    100% |################################| 204kB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauth2client<4.0dev,>=2.0.0 (from gapic-google-cloud-logging-v2<0.92dev,>=0.91.0->google-cloud-logging<1.5dev,>=1.4.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
      "\u001b[K    100% |################################| 81kB 12.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting proto-google-cloud-logging-v2[grpc]<0.92dev,>=0.91.3 (from gapic-google-cloud-logging-v2<0.92dev,>=0.91.0->google-cloud-logging<1.5dev,>=1.4.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/cc/52c1b363b992546d4658a61eb551c85af94ad424734e6c899fdfc8330811/proto-google-cloud-logging-v2-0.91.3.tar.gz\n",
      "Collecting dill<0.3dev,>=0.2.5 (from google-gax<0.16dev,>=0.15.7->google-cloud-firestore<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/a0/19d4d31dee064fc553ae01263b5c55e7fb93daff03a69debbedee647c5a0/dill-0.2.7.1.tar.gz (64kB)\n",
      "\u001b[K    100% |################################| 71kB 12.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting future<0.17dev,>=0.16.0 (from google-gax<0.16dev,>=0.15.7->google-cloud-firestore<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/2b/8d082ddfed935f3608cc61140df6dcbf0edea1bc3ab52fb6c29ae3e81e85/future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |################################| 829kB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0dev,>=1.0.2 (from google-gax<0.16dev,>=0.15.7->google-cloud-firestore<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/b8/47468178ba19143e89b2da778eed660b84136c0a877224e79cc3c1c3fd32/grpcio-1.12.1-cp35-cp35m-manylinux1_x86_64.whl (9.0MB)\n",
      "\u001b[K    100% |################################| 9.0MB 171kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ply==3.8 (from google-gax<0.16dev,>=0.15.7->google-cloud-firestore<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/e0/430fcdb6b3ef1ae534d231397bee7e9304be14a47a267e82ebcb3323d0b5/ply-3.8.tar.gz (157kB)\n",
      "\u001b[K    100% |################################| 163kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting proto-google-cloud-datastore-v1[grpc]<0.91dev,>=0.90.3 (from gapic-google-cloud-datastore-v1<0.16dev,>=0.15.0->google-cloud-datastore<1.5dev,>=1.4.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/1f/4124f15e1132a2eeeaf616d825990bb1d395b4c2c37362654ea5cd89bb42/proto-google-cloud-datastore-v1-0.90.4.tar.gz\n",
      "Collecting proto-google-cloud-error-reporting-v1beta1[grpc]<0.16dev,>=0.15.3 (from gapic-google-cloud-error-reporting-v1beta1<0.16dev,>=0.15.0->google-cloud-error-reporting<0.29dev,>=0.28.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/a1/94790efc86414a127fdf2c1277530c789b458bdd4b120630d4d84ea04898/proto-google-cloud-error-reporting-v1beta1-0.15.3.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httplib2>=0.9.1 (from oauth2client<4.0dev,>=2.0.0->gapic-google-cloud-logging-v2<0.92dev,>=0.91.0->google-cloud-logging<1.5dev,>=1.4.0->google-cloud==0.32.0->cloud-volume)\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/ce/aa4a385e3e9fd351737fd2b07edaa56e7a730448465aceda6b35086a0d9b/httplib2-0.11.3.tar.gz (215kB)\n",
      "\u001b[K    100% |################################| 225kB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: posix-ipc, psutil, tenacity, inflection, Markdown, pandocfilters, blosc, nose2, gapic-google-cloud-logging-v2, grpc-google-iam-v1, googleapis-common-protos, gapic-google-cloud-datastore-v1, gapic-google-cloud-error-reporting-v1beta1, oauth2client, proto-google-cloud-logging-v2, dill, future, ply, proto-google-cloud-datastore-v1, proto-google-cloud-error-reporting-v1beta1, httplib2\n",
      "  Running setup.py bdist_wheel for posix-ipc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5a/5c/17/d0e2d421abaf3b4097bf9db11108380d734195c6d15c24269d\n",
      "  Running setup.py bdist_wheel for psutil ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fe/33/84/baea3d665de2d1af7e8f827f3883811bba5e4149443ccf8191\n",
      "  Running setup.py bdist_wheel for tenacity ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cf/dd/89/99693d2e2c5954ccd7b7ba4fe9571b385712939d25e584a03a\n",
      "  Running setup.py bdist_wheel for inflection ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9f/fa/4f/ef45a51543bd855e40494298824ad5e750109ad18fa80278ad\n",
      "  Running setup.py bdist_wheel for Markdown ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/34/2d/c2/75f4fd57c1b79e59f66388262ed37b69ceac77162988f8af63\n",
      "  Running setup.py bdist_wheel for pandocfilters ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fc/a0/4c/ec26eaa3f2253144e29941bd4031c317a69595dd0b665a0779\n",
      "  Running setup.py bdist_wheel for blosc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/2a/00/8ff3a1d3d423a3163b93917a72146e85c3a3484ba6574854a5\n",
      "  Running setup.py bdist_wheel for nose2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/66/65/52/e7e6215ebee79eff0f3a60db0e6e6f6d751c3ebbb7a52cdfbd\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-logging-v2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/67/f7/2c/c104c937201a0a2bba168ae7058ce7d10da96d881b504a1ae4\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/62/45/af/649bbf07b6595fda010be1bda667cd56d0444d07afc6f8b687\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-datastore-v1 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/79/a9/0f/1b6929a5f1961d22553efa844ea60c589bffcf6e1bd019f7c9\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-error-reporting-v1beta1 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/19/d9/9e/5d0f178712d65c20ef2e2573b4390f94864819fe0b29d91c8f\n",
      "  Running setup.py bdist_wheel for oauth2client ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-logging-v2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a2/63/53/2c822aa22c24d620ce09f799ff1b38cceaac1ae7698368b0d8\n",
      "  Running setup.py bdist_wheel for dill ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/99/c4/ed/1b64d2d5809e60d5a3685530432f6159d6a9959739facb61f2\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a\n",
      "  Running setup.py bdist_wheel for ply ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/21/c0/f0056cc96847933daa961a19eb59a2ecd0228fdbe3376e7a68\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-datastore-v1 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bd/ce/33/8b769968db3761c42c7a91d8a0dbbafc50acfa0750866c8abd\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-error-reporting-v1beta1 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/76/fc/f8/49959a924b1e0d76264f251ea9ab9d27e6b978acf02cd28bff\n",
      "  Running setup.py bdist_wheel for httplib2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/1b/9c/9e/1f6fdb21dbb1fe6a99101d697f12cb8c1fa96c1587df69adba\n",
      "Successfully built posix-ipc psutil tenacity inflection Markdown pandocfilters blosc nose2 gapic-google-cloud-logging-v2 grpc-google-iam-v1 googleapis-common-protos gapic-google-cloud-datastore-v1 gapic-google-cloud-error-reporting-v1beta1 oauth2client proto-google-cloud-logging-v2 dill future ply proto-google-cloud-datastore-v1 proto-google-cloud-error-reporting-v1beta1 httplib2\n",
      "Installing collected packages: jsonschema, inflection, six, Markdown, pandocfilters, python-jsonschema-objects, posix-ipc, psutil, chardet, certifi, urllib3, idna, requests, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, jmespath, docutils, botocore, s3transfer, boto3, dill, future, grpcio, protobuf, ply, googleapis-common-protos, google-gax, httplib2, oauth2client, proto-google-cloud-logging-v2, gapic-google-cloud-logging-v2, google-api-core, google-cloud-core, google-cloud-logging, google-cloud-translate, google-cloud-bigquery-datatransfer, google-cloud-firestore, grpc-google-iam-v1, google-cloud-pubsub, google-resumable-media, google-cloud-bigquery, google-cloud-storage, google-cloud-videointelligence, google-cloud-dns, google-cloud-spanner, google-cloud-speech, google-cloud-trace, google-cloud-bigtable, google-cloud-vision, google-cloud-resource-manager, google-cloud-monitoring, google-cloud-language, google-cloud-runtimeconfig, proto-google-cloud-datastore-v1, gapic-google-cloud-datastore-v1, google-cloud-datastore, google-cloud-container, proto-google-cloud-error-reporting-v1beta1, gapic-google-cloud-error-reporting-v1beta1, google-cloud-error-reporting, google-cloud, blosc, pbr, mock, coverage, nose2, intern, backports-abc, json5, more-itertools, attrs, pluggy, py, atomicwrites, pytest, tqdm, monotonic, tenacity, cloud-volume\n",
      "  Found existing installation: jsonschema 2.6.0\n",
      "    Uninstalling jsonschema-2.6.0:\n",
      "      Successfully uninstalled jsonschema-2.6.0\n",
      "  Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Found existing installation: pandocfilters 1.4.2\n",
      "    Uninstalling pandocfilters-1.4.2:\n",
      "      Successfully uninstalled pandocfilters-1.4.2\n",
      "Successfully installed Markdown-2.4 atomicwrites-1.1.5 attrs-18.1.0 backports-abc-0.5 blosc-1.4.4 boto3-1.7.40 botocore-1.10.40 cachetools-2.1.0 certifi-2018.4.16 chardet-3.0.4 cloud-volume-0.23.0 coverage-4.5.1 dill-0.2.7.1 docutils-0.14 future-0.16.0 gapic-google-cloud-datastore-v1-0.15.3 gapic-google-cloud-error-reporting-v1beta1-0.15.3 gapic-google-cloud-logging-v2-0.91.3 google-api-core-0.1.4 google-auth-1.5.0 google-cloud-0.32.0 google-cloud-bigquery-0.28.0 google-cloud-bigquery-datatransfer-0.1.1 google-cloud-bigtable-0.28.1 google-cloud-container-0.1.1 google-cloud-core-0.28.1 google-cloud-datastore-1.4.0 google-cloud-dns-0.28.0 google-cloud-error-reporting-0.28.0 google-cloud-firestore-0.28.0 google-cloud-language-1.0.2 google-cloud-logging-1.4.0 google-cloud-monitoring-0.28.1 google-cloud-pubsub-0.30.1 google-cloud-resource-manager-0.28.1 google-cloud-runtimeconfig-0.28.1 google-cloud-spanner-0.29.0 google-cloud-speech-0.30.0 google-cloud-storage-1.6.0 google-cloud-trace-0.17.0 google-cloud-translate-1.3.1 google-cloud-videointelligence-1.0.1 google-cloud-vision-0.29.0 google-gax-0.15.16 google-resumable-media-0.3.1 googleapis-common-protos-1.5.3 grpc-google-iam-v1-0.11.4 grpcio-1.12.1 httplib2-0.11.3 idna-2.7 inflection-0.2.0 intern-0.9.6 jmespath-0.9.3 json5-0.5.1 jsonschema-2.3.0 mock-2.0.0 monotonic-1.5 more-itertools-4.2.0 nose2-0.7.4 oauth2client-3.0.0 pandocfilters-1.2 pbr-4.0.4 pluggy-0.6.0 ply-3.8 posix-ipc-1.0.4 proto-google-cloud-datastore-v1-0.90.4 proto-google-cloud-error-reporting-v1beta1-0.15.3 proto-google-cloud-logging-v2-0.91.3 protobuf-3.6.0 psutil-5.4.3 py-1.5.3 pyasn1-0.4.3 pyasn1-modules-0.2.1 pytest-3.6.1 python-jsonschema-objects-0.2.1 requests-2.19.1 rsa-3.4.2 s3transfer-0.1.13 six-1.10.0 tenacity-4.4.0 tqdm-4.23.4 urllib3-1.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scikit-image\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/2c/94aec7f4487f78c365ec7c0669844f7149c62c27c52b1ed072c10d73ba92/scikit_image-0.14.0-cp35-cp35m-manylinux1_x86_64.whl (25.2MB)\n",
      "\u001b[K    100% |################################| 25.3MB 56kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=1.8 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/42/f951cc6838a4dff6ce57211c4d7f8444809ccbe2134179950301e5c4c83c/networkx-2.1.zip (1.6MB)\n",
      "\u001b[K    100% |################################| 1.6MB 835kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dask[array]>=0.9.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/e3/e425e560a9746bf7e99ddc3c6b725067e09f7363f307f538b6fdd752b8d1/dask-0.18.0-py2.py3-none-any.whl (625kB)\n",
      "\u001b[K    100% |################################| 634kB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=0.4.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/1f/203c8886c7b94286ddbbf418779f4303774b65c20474a8b554598c483e90/PyWavelets-0.5.2-cp35-cp35m-manylinux1_x86_64.whl (5.7MB)\n",
      "\u001b[K    100% |################################| 5.7MB 277kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image)\n",
      "Collecting cloudpickle>=0.2.1 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/bf/60ae7ec1e8c6742d2abbb6819c39a48ee796793bcdb7e1d5e41a3e379ddd/cloudpickle-0.5.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.5/dist-packages (from networkx>=1.8->scikit-image)\n",
      "Collecting toolz>=0.7.3; extra == \"array\" (from dask[array]>=0.9.0->scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/d0/a73c15bbeda3d2e7b381a36afb0d9cd770a9f4adc5d1532691013ba881db/toolz-0.9.0.tar.gz (45kB)\n",
      "\u001b[K    100% |################################| 51kB 10.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0; extra == \"array\" in /usr/local/lib/python3.5/dist-packages (from dask[array]>=0.9.0->scikit-image)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image)\n",
      "Building wheels for collected packages: networkx, toolz\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/44/c0/34/6f98693a554301bdb405f8d65d95bbcd3e50180cbfdd98a94e\n",
      "  Running setup.py bdist_wheel for toolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f4/0c/f6/ce6b2d1aa459ee97cc3c0f82236302bd62d89c86c700219463\n",
      "Successfully built networkx toolz\n",
      "Installing collected packages: networkx, toolz, dask, PyWavelets, cloudpickle, scikit-image\n",
      "Successfully installed PyWavelets-0.5.2 cloudpickle-0.5.3 dask-0.18.0 networkx-2.1 scikit-image-0.14.0 toolz-0.9.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.cloudvolume/secrets\n",
    "!cp /notebooks/*.json ~/.cloudvolume/secrets/\n",
    "!pip install cloud-volume\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from cloudvolume import CloudVolume\n",
    "from cloudvolume.lib import Vec, Bbox\n",
    "from skimage.transform import resize\n",
    "\n",
    "def invert_mask(mask, th=50/255.0, bias=0.2):\n",
    "        bitmask = mask < th\n",
    "        th_mask = bitmask * mask\n",
    "        th_max = np.max(th_mask) + bias\n",
    "        return (th_max - th_mask) / th_max\n",
    "\n",
    "class Stack():\n",
    "    \"\"\"Collection of CloudVolume paths, with load & save operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "    def load(self, key, bbox, src_mip=0, dst_mip=0):\n",
    "        src_cv = CloudVolume(self.paths[key], mip=src_mip, fill_missing=True)\n",
    "        src_bbox = Bbox.from_slices(src_cv.slices_from_global_coords(bbox))\n",
    "        img = src_cv[src_bbox.to_slices()][:,:,0,0]\n",
    "        if dst_mip != src_mip:\n",
    "                dst_cv = CloudVolume(self.paths[key], mip=dst_mip, fill_missing=True)\n",
    "                dst_bbox = Bbox.from_slices(dst_cv.slices_from_global_coords(bbox))\n",
    "                img = resize(img, dst_bbox.size3()[:2], preserve_range=True)\n",
    "        return img\n",
    "\n",
    "    def save(self, key, mip, bbox, img, is_field=False):\n",
    "        \"\"\"Save vector field to CloudVolume\n",
    "\n",
    "        Args:\n",
    "        * cv: CloudVolume object\n",
    "        * mip: int representing mip-level of the CloudVolume\n",
    "        * bbox: Bbox at mip 0 with size3 of (x,y,1)\n",
    "        * img: nxm int64 ndarray with shape (x,y)\n",
    "        * is_field: boolean whether img is a field object\n",
    "                field object is 4d tensor with shape (1,x,y,2)\n",
    "        \"\"\"\n",
    "        src_cv = CloudVolume(self.paths['image'], mip=mip)\n",
    "        info = src_cv.info\n",
    "        if is_field:\n",
    "                info['data_type'] = 'float32'\n",
    "                info['num_channels'] = 2\n",
    "        cv = CloudVolume(self.paths[key], mip=mip, bounded=False, \n",
    "                                         info=info, fill_missing=True, \n",
    "                                         non_aligned_writes=True, cdn_cache=False, progress=False)\n",
    "        cv.commit_info()\n",
    "\n",
    "        if is_field:\n",
    "                s = cv.slices_from_global_coords(bbox) \n",
    "                s += (slice(0,2),)\n",
    "                cv[s] = img.permute(1,2,0,3).data.cpu().numpy()\n",
    "        else:\n",
    "                s = cv.slices_from_global_coords(bbox)\n",
    "                cv[s] = img[:,:,np.newaxis,np.newaxis]\n",
    "\n",
    "class Optimizer():\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes:\n",
    "        * src_image: nxm float64 ndarry normalized between [0,1]\n",
    "            This is the image to be transformed by the returned vector field.\n",
    "        * dst_images: list of nxm float64 ndarrays normalized between [0,1]\n",
    "            This is the set of images that transformed src_image will be compared to.\n",
    "        * src_smooth_mask: nxm float64 ndarray normalized between [0,1]\n",
    "            The weight represents that degree to which a pixel participates in smooth\n",
    "            deformation (0: not at all; 1: completely). This mask is used to reduce \n",
    "            the smoothness penalty for pixels that participate in a non-smooth \n",
    "            deformation.\n",
    "            ### this mask is too severe on the smoothness penalty. consider changing\n",
    "        * src_loss_mask: nxm float64 ndarray normalized between [0,1]\n",
    "            This mask is transformed by the vector field and used to reduce the MSE\n",
    "            for pixels that participate in a defect.\n",
    "        * dst_masks_loss: list of nxm float64 ndarrays normalized between [0,1]\n",
    "            Exactly like the src_mask_loss above. These masks are only used to reduce the \n",
    "            MSE for pixels that participate in a defect.\n",
    "            ### unclear what `dst_masks_loss` is ???\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stack, bbox, dst_increment, image_mip, fold_mip, crack_mip, loss_mip, output_mip):\n",
    "        self.stack = stack # the stack of images\n",
    "        self.bbox = bbox # the \"frame\" in which to process\n",
    "        self.image_mip = image_mip # lowest mip at which to process\n",
    "        self.output_mip = output_mip # mip at which to produce the output image\n",
    "        self.src_image = stack.load('image', bbox, src_mip=image_mip, dst_mip=image_mip) / 255.0\n",
    "        self.src_fold = stack.load('fold', bbox, src_mip=fold_mip, dst_mip=image_mip) / 255.0\n",
    "        self.src_crack = stack.load('crack', bbox, src_mip=crack_mip, dst_mip=image_mip) / 255.0\n",
    "#         self.src_smooth_mask = invert_mask(np.minimum(self.src_fold, self.src_crack))\n",
    "#         self.src_smooth_mask = (512 - stack.load('widened', bbox, src_mip=image_mip, dst_mip=image_mip)) / 512.0\n",
    "        self.src_smooth_mask = (255 - stack.load('widened', bbox, src_mip=image_mip, dst_mip=image_mip)) / 255.0\n",
    "#         self.src_smooth_mask = stack.load('widened', bbox, src_mip=image_mip, dst_mip=image_mip)\n",
    "        self.src_loss_mask = (stack.load('loss', bbox, src_mip=loss_mip, dst_mip=image_mip) == 0) * 1.0\n",
    "        \n",
    "        print('min, max:', np.min(self.src_smooth_mask), np.max(self.src_smooth_mask))\n",
    "        \n",
    "        self.dst_images = []\n",
    "        self.dst_masks = []\n",
    "        for z in dst_increment: # run through the layers being compared/aligned to\n",
    "            dst_bbox = self.bbox + [0,0,z]\n",
    "            dst_image = stack.load('image', dst_bbox, src_mip=image_mip, dst_mip=image_mip)\n",
    "            self.dst_images.append(dst_image)\n",
    "            dst_mask = (stack.load('loss', dst_bbox, src_mip=loss_mip, dst_mip=image_mip) == 0) * 1.0\n",
    "            self.dst_masks.append(dst_mask)\n",
    "            if output_mip != image_mip:\n",
    "                dst_image = stack.load('image', dst_bbox, src_mip=output_mip, dst_mip=output_mip)\n",
    "            self.stack.save('output', output_mip, dst_bbox, dst_image, is_field=False)\n",
    "\n",
    "        self.identities = {}\n",
    "    \n",
    "    # crops the image var by d/2 pixels on each edge of the dimensions specified by dims\n",
    "    @staticmethod\n",
    "    def center(var, dims, d):\n",
    "        if not isinstance(d, collections.Sequence):\n",
    "            d = [d for i in range(len(dims))]\n",
    "        for idx, dim in enumerate(dims):\n",
    "            if d[idx] == 0:\n",
    "                continue\n",
    "            var = var.narrow(dim, int(d[idx]/2), int(var.size()[dim] - d[idx]))\n",
    "        return var\n",
    "    \n",
    "    # returns a grid containing the relative pixel location (unity under grid_sample())\n",
    "    def get_identity_grid(self, dim, cache=True):\n",
    "        if self is None or dim not in self.identities:\n",
    "            gx, gy = np.linspace(-1, 1, dim), np.linspace(-1, 1, dim)\n",
    "            I = np.stack(np.meshgrid(gx, gy))\n",
    "            I = np.expand_dims(I, 0)\n",
    "            I = torch.FloatTensor(I)\n",
    "            I = torch.autograd.Variable(I, requires_grad=False)\n",
    "            I = I.permute(0,2,3,1).cuda()\n",
    "            if self is None:\n",
    "                return I\n",
    "            self.identities[dim] = I\n",
    "        if cache:\n",
    "            return self.identities[dim]\n",
    "        else:\n",
    "            return self.identities[dim].clone()\n",
    "\n",
    "    def jacob(self, fields, symmetric = True):\n",
    "        # field (batch, x loc, y loc, component)\n",
    "        ### TODO include difference with zero at edges to encourage no change there\n",
    "        def dx(f):\n",
    "            p = Variable(torch.zeros((1,1,f.size(1),2))).cuda() # padding to maintain shape\n",
    "            if symmetric:\n",
    "                return torch.cat((p, f[:,2:,:,:] - f[:,:-2,:,:], p), 1)\n",
    "            else:\n",
    "                return torch.cat((f[:,1:,:,:] - f[:,:-1,:,:], p), 1)\n",
    "        def dy(f):\n",
    "            p = Variable(torch.zeros((1,f.size(1),1,2))).cuda()\n",
    "            if symmetric:\n",
    "                return torch.cat((p, f[:,:,2:,:] - f[:,:,:-2,:], p), 2)\n",
    "            else:\n",
    "                return torch.cat((f[:,:,1:,:] - f[:,:,:-1,:], p), 2)\n",
    "        fields = sum(map(lambda f: [dx(f), dy(f)], fields), [])\n",
    "        field = torch.sum(torch.cat(fields, -1) ** 2, -1)\n",
    "        return field\n",
    "\n",
    "    def isaac(self, fields, symmetric = True, mip=0):\n",
    "        # field (batch, x loc, y loc, component)\n",
    "        # note assumes that fields has a square shape\n",
    "        ### TODO include difference with zero at edges to encourage no change there\n",
    "        d = 0.1/fields[0].size(1)\n",
    "        def dx(f):\n",
    "            p = Variable(torch.zeros((1,1,f.size(1),2))).cuda() # padding to maintain shape\n",
    "            if symmetric:\n",
    "                return torch.cat((p, (f[:,2:,:,:] - f[:,:-2,:,:]) - 0*d, p), 1)\n",
    "            else:\n",
    "                return torch.cat(((f[:,1:,:,:] - f[:,:-1,:,:]) - 0*d, p), 1)\n",
    "        def dy(f):\n",
    "            p = Variable(torch.zeros((1,f.size(1),1,2))).cuda()\n",
    "            if symmetric:\n",
    "                return torch.cat(((p, f[:,:,2:,:] - f[:,:,:-2,:]) - 0*d, p), 2)\n",
    "            else:\n",
    "                return torch.cat(((f[:,:,1:,:] - f[:,:,:-1,:]) - 0*d, p), 2)\n",
    "        fields = sum(map(lambda f: [dx(f), dy(f)], fields), [])\n",
    "        field = torch.sum(torch.cat(fields, -1) ** 2, -1) - d/5\n",
    "        return field\n",
    "\n",
    "    def penalty(self, fields, mask=1, mip=0):\n",
    "        jacob = self.jacob(fields, symmetric=False)\n",
    "        isaac = self.isaac(fields, symmetric=False, mip=mip)\n",
    "        penalty = torch.mul(jacob, mask) + torch.mul(isaac, 1.0 - mask)\n",
    "        return penalty\n",
    "\n",
    "    def render(self, field):\n",
    "        ### why 255? Is 1.0 a valid value then? Why divide it at all and not just convert to float?\n",
    "        src = stack.load('image', bbox, src_mip=self.output_mip, dst_mip=self.output_mip) / 255.0\n",
    "        src = torch.FloatTensor(src).cuda()\n",
    "        src = Variable(src).unsqueeze(0).unsqueeze(0) # unsqueeze adds a dimension to the tensor\n",
    "        ### a bit cluttered and redundant since check done twice and no need for lambda func. clean up\n",
    "        if self.output_mip < self.image_mip: # upsample the field if needed\n",
    "            upsample = lambda x: nn.Upsample(scale_factor=2**x, mode='bilinear') if x >0 else (lambda y: y)\n",
    "            field = upsample(self.image_mip - self.output_mip)(field.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        y = F.grid_sample(src, field + self.get_identity_grid(field.size(2)))\n",
    "        img = y.data.cpu().numpy()\n",
    "        img = (img[0,0,:,:]*255).astype(np.uint8)\n",
    "        self.stack.save('output', self.output_mip, self.bbox, img, is_field=False)\n",
    "\n",
    "    def process(self, field=None, crop=0, write_field=True, ndownsamples=4, \n",
    "                                                                     currn=5, avgn=20, lambda1=0.4, lr=0.2, \n",
    "                                                                     eps=0.01, min_iter=20, max_iter=1000,\n",
    "                                                                     write_penalty = True):\n",
    "        \"\"\"Compute vector field that minimizes mean squared error (MSE) between \n",
    "        transformed src_image & all dst_images regularized by the smoothness of the \n",
    "        vector field subject to masks that allow the vector field to not be smooth. \n",
    "        The minimization uses SGD.\n",
    "\n",
    "        Args:\n",
    "        * field: nxmx2 float64 torch tensor normalized between [0,1]\n",
    "            This field represents the derived vector field that transforms the \n",
    "            src_image subject to the contraints of the minimization.\n",
    "        * ndownsamples: int\n",
    "            The number of mip layers to use in the alignment process\n",
    "        * lr: float\n",
    "            The learning rate at the lowest mip level (adjusted at higher mips)\n",
    "        * currn and avgn: int\n",
    "            The number of current and prior samples of the cost function to compare for stoppage\n",
    "        * crop: int\n",
    "            crops the returned vector field by this many pixels on each side\n",
    "\n",
    "        Returns:\n",
    "        * field: nxmx2 float64 torch tensor normalized between [0,1]\n",
    "        \"\"\"\n",
    "        downsample = lambda x: nn.AvgPool2d(2**x,2**x, count_include_pad=False) if x > 0 else (lambda y: y)\n",
    "        upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        # convert images to PyTorch Variables\n",
    "        s = torch.FloatTensor(self.src_image)\n",
    "        # note:unsqueeze adds a dimension to the tensor\n",
    "        src_var = Variable((s - torch.mean(s)) / torch.std(s)).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        src_smooth_mask_var = Variable(torch.FloatTensor(self.src_smooth_mask)).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        src_loss_mask_var = Variable(torch.FloatTensor(self.src_loss_mask)).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        dst_vars = []\n",
    "        dst_mask_vars = []\n",
    "        for d, m in zip(self.dst_images, self.dst_masks):\n",
    "            d = torch.FloatTensor(d)\n",
    "            dst_var = Variable((d - torch.mean(d)) / torch.std(d)).cuda().unsqueeze(0).unsqueeze(0)\n",
    "            dst_vars.append(dst_var)\n",
    "            mask_var = Variable(torch.FloatTensor(m)).cuda().unsqueeze(0).unsqueeze(0)\n",
    "            dst_mask_vars.append(mask_var)\n",
    "        \n",
    "        # initialize the field\n",
    "        dim = int(src_var.size()[-1] / (2 ** (ndownsamples - 1)))\n",
    "        if field is None:\n",
    "            # to zeros\n",
    "            field = Variable(torch.zeros((1,dim,dim,2))).cuda().detach()\n",
    "#             # to random values\n",
    "#             field = torch.rand((1,dim,dim,2))\n",
    "#             f_mask = self.get_identity_grid(field.size(2)) * self.get_identity_grid(field.size(2))\n",
    "# #             pad = 64/(2**(ndownsamples - 1))\n",
    "# #             padding = (pad, pad, pad, pad)\n",
    "# #             F.pad(self.center(field, (1,2), 128/(2**(ndownsamples - 1))),padding)\n",
    "#             field = Variable(field).cuda().detach()\n",
    "#             field = field * f_mask\n",
    "        else:\n",
    "            field = Variable(field).cuda().detach()\n",
    "        field.requires_grad = True\n",
    "        \n",
    "        updates = 0\n",
    "        for k in reversed(range(ndownsamples)): # mip levels (image_mip+ndownsamples-1) through image_mip\n",
    "            src_ = downsample(k)(src_var).detach()\n",
    "            src_.requires_grad = False    ### isn't this redundant? detach already sets this\n",
    "            src_smooth_mask_ = downsample(k)(src_smooth_mask_var).detach()\n",
    "            src_smooth_mask_.requires_grad = False\n",
    "            src_loss_mask_ = downsample(k)(src_loss_mask_var).detach()\n",
    "            src_loss_mask_.requires_grad = False\n",
    "            dst_list_, dst_mask_list_ = [], []\n",
    "            for d, m in zip(dst_vars, dst_mask_vars):\n",
    "                dst_ = downsample(k)(d).detach()\n",
    "                dst_.requires_grad = False\n",
    "                dst_list_.append(dst_)\n",
    "                mask_ = downsample(k)(m).detach()\n",
    "                mask_.requires_grad = False\n",
    "                dst_mask_list_.append(mask_)\n",
    "            field = field.detach() ### why, if already detached above?\n",
    "            field.requires_grad = True # not redundant (detach makes it False by default)\n",
    "            opt = torch.optim.SGD([field], lr=lr/(k+1)) ### why is the learning rate increased for lower mips??\n",
    "            #sched = lr_scheduler.StepLR(opt, step_size=1, gamma=0.995)\n",
    "            costs = [] # list of decreasing costs over time\n",
    "            start_updates = updates ### not used (?)\n",
    "            print('Downsample factor {0}x'.format(2**k))\n",
    "            while True:\n",
    "                updates += 1\n",
    "                I = self.get_identity_grid(field.size(2))\n",
    "                pred = F.grid_sample(src_, field + I)\n",
    "#                 print(\"image_mip + k:\", image_mip + k)\n",
    "                pred_loss_mask = F.grid_sample(src_loss_mask_, field + I)\n",
    "                pred_smooth_mask = F.grid_sample(src_smooth_mask_, field + I)\n",
    "                ### why is 128 hardcoded?\n",
    "                # squeeze removes a dimension from the tensor\n",
    "#                 centered_mask = self.center(pred_smooth_mask.squeeze(0), (1,2), 128/(2**k))\n",
    "                centered_mask = self.center(pred_loss_mask.squeeze(0), (1,2), 128/(2**k))\n",
    "                centered_field = self.center(field, (1,2), 128/(2**k))\n",
    "                # calaculate the smoothness penalty\n",
    "                ### currently the masks are transformed with the field, so if the field is wrong,\n",
    "                ### so is the mask (!)\n",
    "                penalty_error_image = self.penalty([centered_field], centered_mask, mip=image_mip+k)\n",
    "                penalty1 = torch.sum(penalty_error_image)\n",
    "                cost = penalty1 * lambda1/(k+1) ### why divide by (k+1) ?\n",
    "                for d, m in zip(dst_list_, dst_mask_list_):\n",
    "                    # merge the pixel-perfect mask from the neighboring slice with that of the current slice\n",
    "                    mask = torch.mul(pred_loss_mask, m) # all 0.0 or 1.0:   0 := ignore\n",
    "                    mse = torch.mul(pred - d, mask)**2 # mse with the neighboring slice\n",
    "                    ### why -1,-2? what are the dimensions of mse?\n",
    "                    mse_error_image = self.center(mse, (-1,-2), 128 / (2**k))\n",
    "                    cost += torch.mean(mse_error_image)\n",
    "#                     print (\"penalty_error_image: \", penalty_error_image.size())\n",
    "#                     print (\"mse_error_image: \", mse_error_image.size())\n",
    "#                     penalty_error_image += mse_error_image\n",
    "                #cost = diff + penalty1 * lambda1/(k+1)\n",
    "#                 print(cost.data.cpu().numpy())\n",
    "                costs.append(cost)\n",
    "                cost.backward()\n",
    "                opt.step()\n",
    "                #sched.step()uniform\n",
    "                opt.zero_grad()\n",
    "                if len(costs) > avgn + currn and len(costs)>min_iter:\n",
    "                    ### can already break here if len(costs)>max_iter\n",
    "                    hist_costs = costs[-(avgn+currn):-currn]\n",
    "                    hist = sum(hist_costs).data[0] / avgn\n",
    "                    curr = sum(costs[-currn:]).data[0] / currn\n",
    "                    ### why does the epsilon increase for lower mip levels? Is that because of larger cost?\n",
    "                    if abs((hist-curr)/(hist+.0000001)) < eps/(2**k) or len(costs)>max_iter:\n",
    "                        ### may be useful to know which of these cases happens. maybe print\n",
    "                        break\n",
    "                #print (downsamples, updates - start_updates)\n",
    "            if write_field:\n",
    "                self.stack.save('field', self.image_mip+k, self.bbox, field + I, is_field=True)\n",
    "            if write_penalty:\n",
    "                # output the smoothness penalty\n",
    "#                 print(\"before:\", penalty_error_image.size())\n",
    "                img = penalty_error_image.squeeze(0).data.cpu().numpy()\n",
    "                img_max = np.max(img) # max value to rescale the image\n",
    "                print(\"smoothness penalty img_max:\", img_max)\n",
    "                img = (img*255/img_max).astype(np.uint8)\n",
    "                v = Vec(64*(2**image_mip),64*(2**image_mip),0)\n",
    "                bbox = Bbox(self.bbox.minpt + v, self.bbox.maxpt - v)\n",
    "#                 print (\"self:\", self.bbox)\n",
    "#                 print (\"modified:\", bbox)\n",
    "                self.stack.save('penalty', self.image_mip+k, bbox, img, is_field=False)\n",
    "                # output the mse\n",
    "                img = mse_error_image.squeeze(0).squeeze(0).data.cpu().numpy()\n",
    "                img_max = np.max(img) # max value to rescale the image\n",
    "                print(\"mse img_max:\", img_max)\n",
    "                img = (img*255/img_max).astype(np.uint8)\n",
    "                v = Vec(64*(2**image_mip),64*(2**image_mip),0)\n",
    "                bbox = Bbox(self.bbox.minpt + v, self.bbox.maxpt - v)\n",
    "                self.stack.save('mse', self.image_mip+k, bbox, img, is_field=False)\n",
    "            if k > 0:\n",
    "                field = upsample(field.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        #print(cost.data[0], diff.data[0], penalty1.data[0])\n",
    "        print(cost.data.cpu().numpy())\n",
    "        print('done:', updates)\n",
    "        print(field.shape)\n",
    "        # return self.center(field, (1,2), crop*2).data.cpu().numpy()[0]\n",
    "        return self.center(field, (1,2), crop*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min, max: 0.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsample factor 4x\n",
      "smoothness penalty img_max: 0.00019135232\n",
      "mse img_max: 6.7251077\n",
      "Downsample factor 2x\n",
      "smoothness penalty img_max: 9.593552e-05\n",
      "mse img_max: 11.771144\n",
      "Downsample factor 1x\n",
      "smoothness penalty img_max: 3.5555724e-05\n",
      "mse img_max: 21.087471\n",
      "[0.22231531]\n",
      "done: 2246\n",
      "torch.Size([1, 512, 512, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min, max: 0.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsample factor 4x\n",
      "smoothness penalty img_max: 0.000447849\n",
      "mse img_max: 9.715247\n",
      "Downsample factor 2x\n",
      "smoothness penalty img_max: 0.00014609974\n",
      "mse img_max: 11.840547\n",
      "Downsample factor 1x\n",
      "smoothness penalty img_max: 2.5572153e-05\n",
      "mse img_max: 16.032907\n",
      "[-1.5268309]\n",
      "done: 3688\n",
      "torch.Size([1, 512, 512, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min, max: 1.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsample factor 4x\n",
      "smoothness penalty img_max: 0.00011854041\n",
      "mse img_max: 8.876527\n",
      "Downsample factor 2x\n",
      "smoothness penalty img_max: 4.6463017e-05\n",
      "mse img_max: 13.303036\n",
      "Downsample factor 1x\n",
      "smoothness penalty img_max: 1.1455632e-05\n",
      "mse img_max: 15.666734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7916536]\n",
      "done: 2026\n",
      "torch.Size([1, 512, 512, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'optimizertests_grad_init_2'\n",
    "paths = {\n",
    "    'image': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/unmasked',\n",
    "    'fold': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/fold_detector_v1',\n",
    "    'crack': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/crack_detector_v3',\n",
    "    'loss': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/ground_truth',\n",
    "    'output': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}'.format(name),\n",
    "    'field': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/field'.format(name),\n",
    "    'penalty': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/penalty'.format(name),\n",
    "    'mse': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/mse'.format(name),\n",
    "    'widened': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/widened'.format(name),\n",
    "}\n",
    "stack = Stack(paths)\n",
    "offsets = [\n",
    "              Vec(102716, 107077, 526), # fold\n",
    "              Vec(102320, 111106, 526), # crack\n",
    "#               Vec(102565, 103904, 527), # no defect\n",
    "              Vec(104799, 109774, 526) # test (remove later)\n",
    "          ]\n",
    "size = Vec(2048, 2048, 1)\n",
    "image_mip = 2\n",
    "fold_mip = 5\n",
    "crack_mip = 5\n",
    "loss_mip = 2\n",
    "output_mip = 1\n",
    "dst_increment = [-1,1]\n",
    "for offset in offsets:\n",
    "    bbox = Bbox(offset, offset+size)\n",
    "    o = Optimizer(stack, bbox, dst_increment, image_mip, fold_mip, crack_mip, loss_mip, output_mip)\n",
    "    field = o.process(ndownsamples=3, currn=5, avgn=20, lambda1=3.6, lr=0.025, \n",
    "                                                  eps=0.001, min_iter=100, max_iter=5000, \n",
    "                                                  field=field_init.data.cpu()\n",
    "                                                  )\n",
    "    o.render(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 53.404799164265825, 74.6877707974304, 53.404799164265825, 0.0],\n",
       " [53.404799164265825, 127.5, 164.8438853987152, 127.5, 53.404799164265825],\n",
       " [74.6877707974304,\n",
       "  164.8438853987152,\n",
       "  255.0,\n",
       "  164.8438853987152,\n",
       "  74.6877707974304],\n",
       " [53.404799164265825, 127.5, 164.8438853987152, 127.5, 53.404799164265825],\n",
       " [0.0, 53.404799164265825, 74.6877707974304, 53.404799164265825, 0.0]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "kernel_size = 101\n",
    "# kernel = np.array([[np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt(np.sqrt((i-kernel_size//2)**2 + (j-kernel_size//2)**2)))))) for j in range(kernel_size)] for i in range(kernel_size)])\n",
    "kernel = np.array([[np.sqrt((i-kernel_size//2)**2 + (j-kernel_size//2)**2) for j in range(kernel_size)] for i in range(kernel_size)])\n",
    "kernel = 255 - kernel/np.max(kernel)*255\n",
    "[[kernel[i,j] for i in range(0,kernel_size,kernel_size//4)] for j in range(0,kernel_size,kernel_size//4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# widen pixel-perfect mask\n",
    "name = 'optimizertests_grad_init_2'\n",
    "paths = {\n",
    "    'image': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/unmasked',\n",
    "    'fold': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/fold_detector_v1',\n",
    "    'crack': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/crack_detector_v3',\n",
    "    'loss': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/ground_truth',\n",
    "    'output': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}'.format(name),\n",
    "    'field': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/field'.format(name),\n",
    "    'penalty': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/penalty'.format(name),\n",
    "    'mse': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/mse'.format(name),\n",
    "    'widened': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/widened'.format(name),\n",
    "}\n",
    "stack = Stack(paths)\n",
    "offsets = [\n",
    "              Vec(102716, 107077, 526), # fold\n",
    "              Vec(102320, 111106, 526), # crack\n",
    "#               Vec(102565, 103904, 527), # no defect\n",
    "              Vec(104799, 109774, 526) # test (remove later)\n",
    "          ]\n",
    "size = Vec(2048, 2048, 1)\n",
    "image_mip = 2\n",
    "fold_mip = 5\n",
    "crack_mip = 5\n",
    "loss_mip = 2\n",
    "output_mip = 1\n",
    "dst_increment = [-1,1]\n",
    "\n",
    "for offset in offsets:\n",
    "    bbox = Bbox(offset, offset+size)\n",
    "    pixelperfect_mask = stack.load('loss', bbox, src_mip=loss_mip, dst_mip=image_mip).astype(np.float64)\n",
    "    widened = ndimage.filters.convolve(pixelperfect_mask, kernel, mode='constant', cval=0.0)\n",
    "    widened = (widened / (np.max(widened)+0.0)) * (1.0 - pixelperfect_mask) + pixelperfect_mask\n",
    "    widened = (widened * 255.0).astype(np.uint8)\n",
    "    stack.save('widened', loss_mip, bbox, widened, is_field=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]] 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.0189644  -0.01907803 -0.01930539 ...  0.          0.\n",
      "    0.        ]\n",
      "  [-0.01870638 -0.01881982 -0.01904681 ...  0.          0.\n",
      "    0.        ]\n",
      "  [-0.01844705 -0.01856029 -0.01878689 ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ... -0.00043125 -0.00042975\n",
      "   -0.00042899]\n",
      "  [ 0.          0.          0.         ... -0.00042323 -0.00042176\n",
      "   -0.00042102]\n",
      "  [ 0.          0.          0.         ... -0.00041534 -0.0004139\n",
      "   -0.00041317]]\n",
      "\n",
      " [[ 0.00111825  0.00137627  0.00163468 ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.00099164  0.00125031  0.00150938 ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.00073712  0.0009971   0.00125748 ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ... -0.00078962 -0.00078155\n",
      "   -0.00077352]\n",
      "  [ 0.          0.          0.         ... -0.00077796 -0.00077002\n",
      "   -0.00076211]\n",
      "  [ 0.          0.          0.         ... -0.00077216 -0.00076428\n",
      "   -0.00075643]]] 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
      "Downloading:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]] nan\n"
     ]
    }
   ],
   "source": [
    "from skimage.filters import gaussian\n",
    "\n",
    "# initialize vector field to gradient of gaussian mask\n",
    "name = 'optimizertests_grad_init_2'\n",
    "paths = {\n",
    "    'image': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/unmasked',\n",
    "    'fold': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/fold_detector_v1',\n",
    "    'crack': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/crack_detector_v3',\n",
    "    'loss': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/mask/ground_truth',\n",
    "    'output': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}'.format(name),\n",
    "    'field': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/field'.format(name),\n",
    "    'penalty': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/penalty'.format(name),\n",
    "    'mse': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/mse'.format(name),\n",
    "    'widened': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/widened'.format(name),\n",
    "    'expand': 'gs://neuroglancer/basil_v0/son_of_alignment/v3.04/optimizer_tests_bn/{0}/expand'.format(name),\n",
    "}\n",
    "stack = Stack(paths)\n",
    "offsets = [\n",
    "              Vec(102716, 107077, 526), # fold\n",
    "              Vec(102320, 111106, 526), # crack\n",
    "#               Vec(102565, 103904, 527), # no defect\n",
    "              Vec(104799, 109774, 526) # test (remove later)\n",
    "          ]\n",
    "size = Vec(2048, 2048, 1)\n",
    "image_mip = 2\n",
    "fold_mip = 5\n",
    "crack_mip = 5\n",
    "loss_mip = 2\n",
    "output_mip = 1\n",
    "dst_increment = [-1,1]\n",
    "\n",
    "already = False\n",
    "\n",
    "for offset in offsets:\n",
    "    bbox = Bbox(offset, offset+size)\n",
    "    \n",
    "    pixelperfect_mask = stack.load('loss', bbox, src_mip=loss_mip, dst_mip=0).astype(np.float64)\n",
    "\n",
    "    widened = gaussian(pixelperfect_mask, sigma=120)\n",
    "    widened = (widened / (np.max(widened)+0.0) * 255)\n",
    "#     stack.save('widened', 0, bbox, widened.astype(np.uint8), is_field=False)\n",
    "    \n",
    "    image = stack.load('image', bbox, src_mip=image_mip, dst_mip=image_mip).astype(np.float64) / 255.0\n",
    "#     widened = stack.load('widened', bbox, src_mip=loss_mip, dst_mip=loss_mip).astype(np.float64)\n",
    "    \n",
    "    field = np.gradient(widened)\n",
    "    temp = field[0]\n",
    "    field[0] = field[1]\n",
    "    field[1] = temp\n",
    "#     field = field/np.max(field)/50*1.8\n",
    "    field = field/np.max(field)/50*2\n",
    "    field1 = field\n",
    "    print(field1, np.max(field1))\n",
    "#     print(\"field shape:\", np.shape(field))\n",
    "#     print(\"image shape:\", np.shape(image))\n",
    "    \n",
    "    # convert images to PyTorch Variables\n",
    "    s = torch.FloatTensor(image)\n",
    "    # note:unsqueeze adds a dimension to the tensor\n",
    "#     s = (s - torch.mean(s)) / torch.std(s)\n",
    "    image = Variable(s).cuda().unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    field = np.expand_dims(field, 0)\n",
    "    field = torch.FloatTensor(field)\n",
    "    field = torch.autograd.Variable(field, requires_grad=False)\n",
    "    field = field.permute(0,2,3,1).cuda()\n",
    "    \n",
    "    I = Optimizer.get_identity_grid(None,field.size(2))\n",
    "    expand = F.grid_sample(image, field + I)\n",
    "    \n",
    "#     print(\"var field shape:\", np.shape(field))\n",
    "#     print(\"var image shape:\", np.shape(image))\n",
    "#     print(\"var I shape:\", np.shape(I))\n",
    "#     print(\"var expand shape:\", np.shape(expand))\n",
    "    \n",
    "    expand = (expand * 255.0).squeeze(0).squeeze(0).data.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    if not already:\n",
    "        already = True\n",
    "        field_init = nn.AvgPool2d(kernel_size=16, stride=16)(field.permute(0,3,1,2)).detach().permute(0,2,3,1)\n",
    "    \n",
    "    stack.save('expand', 0, bbox, expand, is_field=False)\n",
    "    stack.save('field', 0, bbox, field + I, is_field=True)\n",
    "    \n",
    "    field_plus_I = (field + I)\n",
    "    \n",
    "    field_plus_I = field_plus_I.permute(0,3,1,2)\n",
    "    field_plus_I = nn.AvgPool2d(kernel_size=4, stride=4)(field_plus_I).permute(0,2,3,1)\n",
    "    stack.save('field', 2, bbox, field_plus_I, is_field=True)\n",
    "    \n",
    "    field_plus_I = field_plus_I.permute(0,3,1,2)\n",
    "    field_plus_I = nn.AvgPool2d(kernel_size=4, stride=4)(field_plus_I).permute(0,2,3,1)\n",
    "    stack.save('field', 4, bbox, field_plus_I, is_field=True)\n",
    "    \n",
    "    field_plus_I = field_plus_I.permute(0,3,1,2)\n",
    "    field_plus_I = nn.AvgPool2d(kernel_size=4, stride=4)(field_plus_I).permute(0,2,3,1)\n",
    "    stack.save('field', 6, bbox, field_plus_I, is_field=True)\n",
    "#     stack.save('expand', image_mip, bbox, np.abs(field1[1]*2550).astype(np.uint8), is_field=False)\n",
    "    \n",
    "#     widened = ndimage.filters.convolve(pixelperfect_mask, kernel, mode='constant', cval=0.0)\n",
    "#     widened = (widened / (np.max(widened)+0.0) * 255).astype(np.uint8)\n",
    "#     stack.save('widened', loss_mip, bbox, widened, is_field=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
